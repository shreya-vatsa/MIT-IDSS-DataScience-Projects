{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "AT5OogJVFbwu",
   "metadata": {
    "id": "AT5OogJVFbwu"
   },
   "source": [
    "# ExtraaLearn Project\n",
    "\n",
    "## Context\n",
    "\n",
    "The EdTech industry has been surging in the past decade immensely, and according to a forecast, the Online Education market would be worth $286.62bn by 2023 with a compound annual growth rate (CAGR) of 10.26% from 2018 to 2023. The modern era of online education has enforced a lot in its growth and expansion beyond any limit. Due to having many dominant features like ease of information sharing, personalized learning experience, transparency of assessment, etc, it is now preferable to traditional education. \n",
    "\n",
    "In the present scenario due to the Covid-19, the online education sector has witnessed rapid growth and is attracting a lot of new customers. Due to this rapid growth, many new companies have emerged in this industry. With the availability and ease of use of digital marketing resources, companies can reach out to a wider audience with their offerings. The customers who show interest in these offerings are termed as leads. There are various sources of obtaining leads for Edtech companies, like\n",
    "\n",
    "* The customer interacts with the marketing front on social media or other online platforms. \n",
    "* The customer browses the website/app and downloads the brochure\n",
    "* The customer connects through emails for more information.\n",
    "\n",
    "The company then nurtures these leads and tries to convert them to paid customers. For this, the representative from the organization connects with the lead on call or through email to share further details.\n",
    "\n",
    "## Objective\n",
    "\n",
    "ExtraaLearn is an initial stage startup that offers programs on cutting-edge technologies to students and professionals to help them upskill/reskill. With a large number of leads being generated on a regular basis, one of the issues faced by ExtraaLearn is to identify which of the leads are more likely to convert so that they can allocate resources accordingly. You, as a data scientist at ExtraaLearn, have been provided the leads data to:\n",
    "* Analyze and build an ML model to help identify which leads are more likely to convert to paid customers, \n",
    "* Find the factors driving the lead conversion process\n",
    "* Create a profile of the leads which are likely to convert\n",
    "\n",
    "\n",
    "## Data Description\n",
    "\n",
    "The data contains the different attributes of leads and their interaction details with ExtraaLearn. The detailed data dictionary is given below.\n",
    "\n",
    "\n",
    "**Data Dictionary**\n",
    "* ID: ID of the lead\n",
    "* age: Age of the lead\n",
    "* current_occupation: Current occupation of the lead. Values include 'Professional','Unemployed',and 'Student'\n",
    "* first_interaction: How did the lead first interacted with ExtraaLearn. Values include 'Website', 'Mobile App'\n",
    "* profile_completed: What percentage of profile has been filled by the lead on the website/mobile app. Values include Low - (0-50%), Medium - (50-75%), High (75-100%)\n",
    "* website_visits: How many times has a lead visited the website\n",
    "* time_spent_on_website: Total time spent on the website\n",
    "* page_views_per_visit: Average number of pages on the website viewed during the visits.\n",
    "* last_activity: Last interaction between the lead and ExtraaLearn. \n",
    "    * Email Activity: Seeking for details about program through email, Representative shared information with lead like brochure of program , etc \n",
    "    * Phone Activity: Had a Phone Conversation with representative, Had conversation over SMS with representative, etc\n",
    "    * Website Activity: Interacted on live chat with representative, Updated profile on website, etc\n",
    "\n",
    "* print_media_type1: Flag indicating whether the lead had seen the ad of ExtraaLearn in the Newspaper.\n",
    "* print_media_type2: Flag indicating whether the lead had seen the ad of ExtraaLearn in the Magazine.\n",
    "* digital_media: Flag indicating whether the lead had seen the ad of ExtraaLearn on the digital platforms.\n",
    "* educational_channels: Flag indicating whether the lead had heard about ExtraaLearn in the education channels like online forums, discussion threads, educational websites, etc.\n",
    "* referral: Flag indicating whether the lead had heard about ExtraaLearn through reference.\n",
    "* status: Flag indicating whether the lead was converted to a paid customer or not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dirty-island",
   "metadata": {
    "id": "dirty-island"
   },
   "source": [
    "## Importing necessary libraries and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statewide-still",
   "metadata": {
    "id": "statewide-still"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from statsmodels.tools.sm_exceptions import ConvergenceWarning\n",
    "\n",
    "warnings.simplefilter(\"ignore\", ConvergenceWarning)\n",
    "\n",
    "# Libraries to help with reading and manipulating data\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Library to split data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# libaries to help with data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Removes the limit for the number of displayed columns\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "# Sets the limit for the number of displayed rows\n",
    "pd.set_option(\"display.max_rows\", 200)\n",
    "# setting the precision of floating numbers to 5 decimal points\n",
    "pd.set_option(\"display.float_format\", lambda x: \"%.5f\" % x)\n",
    "\n",
    "# To build model for prediction\n",
    "import statsmodels.stats.api as sms\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tools.tools import add_constant\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# To tune different models\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "# To get diferent metric scores\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import (\n",
    "    f1_score,\n",
    "    accuracy_score,\n",
    "    recall_score,\n",
    "    precision_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    roc_auc_score,\n",
    "    precision_recall_curve,\n",
    "    roc_curve,\n",
    "    make_scorer,\n",
    ")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f013dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset from the local data folder\n",
    "df = pd.read_csv('data/ExtraaLearn.csv')\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nColumn names: {df.columns.tolist()}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desperate-infection",
   "metadata": {
    "id": "desperate-infection"
   },
   "source": [
    "## Data Overview\n",
    "\n",
    "- Observations\n",
    "- Sanity checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "persistent-juice",
   "metadata": {
    "id": "persistent-juice"
   },
   "outputs": [],
   "source": [
    "data = df.copy()\n",
    "data.head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8744e8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0cf76c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f06b25",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "- Missing value treatment (if needed)\n",
    "- Feature engineering (if needed)\n",
    "- Outlier detection and treatment (if needed)\n",
    "- Preparing data for modeling \n",
    "- Any other preprocessing steps (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b7c8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values = data.isnull().sum()\n",
    "missing_values # returns the count of null values by column index and since the values are zero, there are no null values in the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f3cbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.duplicated().sum() #data = data.drop_duplicates() if there are any duplicates encountered. Since there are none encountered, this command is not required.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfaa10dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.nunique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0bc3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop([\"ID\"], axis = 1, inplace = True) #This column is dropped as we determine it is not important in our evaluation due to the uniqueness of each ID.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da1788a",
   "metadata": {},
   "source": [
    "Observations: ID is an identifier which is unique for each lead. We can drop this column since it would provide no value to the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seasonal-calibration",
   "metadata": {
    "id": "seasonal-calibration"
   },
   "source": [
    "## Exploratory Data Analysis (EDA)\n",
    "\n",
    "- EDA is an important part of any project involving data.\n",
    "- It is important to investigate and understand the data better before building a model with it.\n",
    "- A few questions have been mentioned below which will help you approach the analysis in the right manner and generate insights from the data.\n",
    "- A thorough analysis of the data, in addition to the questions mentioned below, should be done."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "approved-brother",
   "metadata": {
    "id": "approved-brother"
   },
   "source": [
    "**Questions**\n",
    "1. Leads will have different expectations from the outcome of the course and the current occupation may play a key role in getting them to participate in the program. Find out how current occupation affects lead status.\n",
    "2. The company's first impression on the customer must have an impact. Do the first channels of interaction have an impact on the lead status? \n",
    "3. The company uses multiple modes to interact with prospects. Which way of interaction works best? \n",
    "4. The company gets leads from various channels such as print media, digital media, referrals, etc. Which of these channels have the highest lead conversion rate?\n",
    "5. People browsing the website or mobile application are generally required to create a profile by sharing their personal data before they can access additional information.Does having more details about a prospect increase the chances of conversion?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "increasing-louisiana",
   "metadata": {
    "id": "increasing-louisiana"
   },
   "outputs": [],
   "source": [
    "#creating numerical columns\n",
    "num_cols = ['age','website_visits','time_spent_on_website','page_views_per_visit']\n",
    "#creating categorical columns\n",
    "cat_cols = ['current_occupation','first_interaction','profile_completed','last_activity','print_media_type1','print_media_type2','digital_media','educational_channels','referral','status']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ca1343",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[num_cols].describe().T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5748cd",
   "metadata": {},
   "source": [
    "Observations: Average lead age is 46.201. With range = 63-18= 45 50% of leads spend 376 seconds on the website. However there are some extreme values given that max is 2537 seconds and min being 0. Average number of pages on the website viewed during the visits is 3.026, and the average number of times a lead has visited the website is 3.567."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10df7c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating histograms\n",
    "data[num_cols].hist(figsize=(14,14))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e4c141",
   "metadata": {},
   "source": [
    "Observations: The number of leads wrt age is skewed left. This indicates that leads are increasing wrt age. And leads wrt website_visits is skewed right.Indicating that the frequency of number of leads decrease as number of visits increase. Time spent on the website has a bimodal structure with peaks at 0-250 then at 1750-2000 seconds spent online, and the central tendency of page views per visit is narrow and skewed slightly to the right. It would be closer to the normal distribution had the variance been greater than current value. The peak for the page_views_per_visit is at 1.875-3.75 pages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71a8f94",
   "metadata": {},
   "source": [
    "Univariate Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bfab0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in cat_cols:\n",
    "    print(data[column].value_counts(normalize = True)) #we can leave the normalize parameter out of the statement is we want to display raw counts.\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05df03a",
   "metadata": {},
   "source": [
    "Observations: The status conversion rate (yes) is 29.86%. Around 57.00% of the leads are of the professional background. Website proportions are greater than mobile app proportions which can hint at a higher engaement level on the website. Out of all the last_activity interactions, the email activity has more leads than its counterparts. Most leads have high-medium profile completion. Out of all the four channels and the referral section, the educational channel has the preferred mode of interaction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66938a49",
   "metadata": {},
   "source": [
    "Bivariate and Multivariate Analysis\n",
    "\n",
    "How is the conversion rate related to other categorical variables?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46bee87",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in cat_cols:\n",
    "    if i!='status':\n",
    "        (pd.crosstab(data[i],data['status'],normalize='index')*100).plot(kind='bar',figsize=(8,4),stacked=True)\n",
    "        plt.ylabel('Percentage Lead Conversion %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788e32ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean of numerical variables grouped by status\n",
    "data.groupby(['status'])[num_cols].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54bac8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the correlation between numerical variables\n",
    "plt.figure(figsize=(15,8))\n",
    "sns.heatmap(data[num_cols].corr(),annot=True, fmt='0.2f', cmap='YlGnBu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de0e105",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_list = data.select_dtypes(include=np.number).columns.tolist()\n",
    "\n",
    "plt.figure(figsize=(12, 7))\n",
    "sns.heatmap(\n",
    "    data[cols_list].corr(), annot=True, vmin=-1, vmax=1, fmt=\".2f\", cmap=\"Spectral\"\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0063a9",
   "metadata": {},
   "source": [
    "Further Univariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1480196e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to plot a boxplot and a histogram along the same scale.\n",
    "\n",
    "def histogram_boxplot(data, feature, figsize=(12, 7), kde=False, bins=None):\n",
    "    \"\"\"\n",
    "    Boxplot and histogram combined\n",
    "\n",
    "    data: dataframe\n",
    "    feature: dataframe column\n",
    "    figsize: size of figure (default (12,7))\n",
    "    kde: whether to the show density curve (default False)\n",
    "    bins: number of bins for histogram (default None)\n",
    "    \"\"\"\n",
    "    f2, (ax_box2, ax_hist2) = plt.subplots(\n",
    "        nrows=2,  # Number of rows of the subplot grid= 2\n",
    "        sharex=True,  # x-axis will be shared among all subplots\n",
    "        gridspec_kw={\"height_ratios\": (0.25, 0.75)},\n",
    "        figsize=figsize,\n",
    "    )  # creating the 2 subplots\n",
    "    sns.boxplot(\n",
    "        data=data, x=feature, ax=ax_box2, showmeans=True, color=\"violet\"\n",
    "    )  # boxplot will be created and a star will indicate the mean value of the column\n",
    "    sns.histplot(\n",
    "        data=data, x=feature, kde=kde, ax=ax_hist2, bins=bins, palette=\"winter\"\n",
    "    ) if bins else sns.histplot(\n",
    "        data=data, x=feature, kde=kde, ax=ax_hist2\n",
    "    )  # For histogram\n",
    "    ax_hist2.axvline(\n",
    "        data[feature].mean(), color=\"green\", linestyle=\"--\"\n",
    "    )  # Add mean to the histogram\n",
    "    ax_hist2.axvline(\n",
    "        data[feature].median(), color=\"black\", linestyle=\"-\"\n",
    "    )  # Add median to the histogram\n",
    "histogram_boxplot(data, \"age\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13f9eca",
   "metadata": {},
   "source": [
    "Observation: Has more outliers given that it is skewed heavily to the right.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b91345",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data[\"website_visits\"] == 0].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffc3cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram_boxplot(data, \"time_spent_on_website\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f070cf82",
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram_boxplot(data, \"page_views_per_visit\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2e99e8",
   "metadata": {},
   "source": [
    "Observation: the number of outliers in this feature is highest amongst all the numerical distributions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8edf0c2",
   "metadata": {},
   "source": [
    "Current Occupation and Lead Status\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aee39c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa6e87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(data=data, x='status', hue='current_occupation')\n",
    "plt.title('Distribution of Lead Status by Current Occupation')\n",
    "plt.xlabel('Lead Status')\n",
    "plt.ylabel('Count')\n",
    "plt.legend(title='Current Occupation', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6538e1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "sns.boxplot(data = data, x = data[\"current_occupation\"], y = data[\"age\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5638dc60",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupby([\"current_occupation\"])[\"age\"].describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07342be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distribution_plot_wrt_target(data, predictor, target):\n",
    "\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "    target_uniq = data[target].unique()\n",
    "\n",
    "    axs[0, 0].set_title(\"Distribution of target for target=\" + str(target_uniq[0]))\n",
    "    sns.histplot(\n",
    "        data=data[data[target] == target_uniq[0]],\n",
    "        x=predictor,\n",
    "        kde=True,\n",
    "        ax=axs[0, 0],\n",
    "        color=\"teal\",\n",
    "        stat=\"density\",\n",
    "    )\n",
    "\n",
    "    axs[0, 1].set_title(\"Distribution of target for target=\" + str(target_uniq[1]))\n",
    "    sns.histplot(\n",
    "        data=data[data[target] == target_uniq[1]],\n",
    "        x=predictor,\n",
    "        kde=True,\n",
    "        ax=axs[0, 1],\n",
    "        color=\"orange\",\n",
    "        stat=\"density\",\n",
    "    )\n",
    "\n",
    "    axs[1, 0].set_title(\"Boxplot w.r.t target\")\n",
    "    sns.boxplot(data=data, x=target, y=predictor, ax=axs[1, 0], palette=\"gist_rainbow\")\n",
    "\n",
    "    axs[1, 1].set_title(\"Boxplot (without outliers) w.r.t target\")\n",
    "    sns.boxplot(\n",
    "        data=data,\n",
    "        x=target,\n",
    "        y=predictor,\n",
    "        ax=axs[1, 1],\n",
    "        showfliers=False,\n",
    "        palette=\"gist_rainbow\",\n",
    "    )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16d65d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "distribution_plot_wrt_target(data, \"time_spent_on_website\", \"status\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8df1c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupby([\"status\"])[\"time_spent_on_website\"].median()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08629ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "distribution_plot_wrt_target(data, \"website_visits\", \"status\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2224cbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "distribution_plot_wrt_target(data, \"page_views_per_visit\", \"status\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697f6118",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversion_rates = data.groupby('first_interaction')['status'].mean()\n",
    "print(conversion_rates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a77a845",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# Create a contingency table\n",
    "contingency_table = pd.crosstab(data['first_interaction'], data['status'])\n",
    "\n",
    "# Perform the chi-squared test\n",
    "chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
    "print(f\"Chi-Squared Value: {chi2}\")\n",
    "print(f\"P-value: {p}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b997020c",
   "metadata": {},
   "source": [
    "There is a significant association between the first channel of interaction and the lead conversion status as the p-value is lower than threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85688e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the conversion rate for each last activity type\n",
    "conversion_rates = data.groupby('last_activity')['status'].mean().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc322954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the conversion rates\n",
    "conversion_rates.plot(kind='bar', figsize=(10, 6))\n",
    "plt.title('Conversion Rate by Last Activity')\n",
    "plt.xlabel('Last Activity')\n",
    "plt.ylabel('Conversion Rate')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af24d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x='last_activity', y='status', data=data, kind='point', aspect=2, ci='sd')\n",
    "plt.title('Conversion Rate by Last Activity')\n",
    "plt.xlabel('Last Activity')\n",
    "plt.ylabel('Conversion Rate (Proportion of Status=1)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900eb448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the conversion rate for each mode of interaction\n",
    "modes_of_interaction = ['print_media_type1', 'print_media_type2', 'digital_media', 'educational_channels', 'referral']\n",
    "# Convert 'Yes'/'No' to 1/0 for each mode\n",
    "for mode in modes_of_interaction:\n",
    "    data[mode] = data[mode].map({'Yes': 1, 'No': 0})\n",
    "# Calculate the conversion rate for each mode\n",
    "conversion_rates = {}\n",
    "for mode in modes_of_interaction:\n",
    "    # Calculate mean only for rows where the mode is 'Yes' (now converted to 1)\n",
    "    rate = data[data[mode] == 1]['status'].mean()\n",
    "    conversion_rates[mode] = rate\n",
    "\n",
    "# Print conversion rates to verify calculations\n",
    "print(conversion_rates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede49878",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the conversion rates to a DataFrame\n",
    "conversion_rates_df = pd.DataFrame(list(conversion_rates.items()), columns=['Interaction_Mode', 'Conversion_Rate'])\n",
    "\n",
    "# Plot the conversion rates\n",
    "plt.figure(figsize=(10, 6))\n",
    "conversion_rates_df = conversion_rates_df.sort_values('Conversion_Rate', ascending=False)\n",
    "plt.barh(conversion_rates_df['Interaction_Mode'], conversion_rates_df['Conversion_Rate'])  # using horizontal bar plot for better readability\n",
    "plt.xlabel('Conversion Rate')\n",
    "plt.ylabel('Mode of Interaction')\n",
    "plt.title('Conversion Rate by Mode of Interaction')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78bc910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that the modes are binary, create the contingency table for the Chi-Squared test\n",
    "contingency_table = pd.crosstab(index=data['status'], columns=[data['print_media_type1'],\n",
    "                                                               data['print_media_type2'],\n",
    "                                                               data['digital_media'],\n",
    "                                                               data['educational_channels'],\n",
    "                                                               data['referral']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2623e8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# Apply the Chi-Squared test\n",
    "chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
    "\n",
    "print(f\"Chi-Squared Value: {chi2}\")\n",
    "print(f\"P-value: {p}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3c45ad",
   "metadata": {},
   "source": [
    "There is a statistically significant difference in conversion rates across the different modes of interaction.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c2c379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming the dataset has a 'profile_completion' column with categories like 'Low', 'Medium', 'High'\n",
    "conversion_rates_by_profile = data.groupby('profile_completed')['status'].mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ff6505",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x=conversion_rates_by_profile.index, y=conversion_rates_by_profile.values)\n",
    "plt.title('Conversion Rate by Profile Completion')\n",
    "plt.xlabel('Profile Completion Level')\n",
    "plt.ylabel('Conversion Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f533722",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import f_oneway\n",
    "\n",
    "# Create groups for each profile completion category\n",
    "group_low = data[data['profile_completed'] == 'Low']['status']\n",
    "group_medium = data[data['profile_completed'] == 'Medium']['status']\n",
    "group_high = data[data['profile_completed'] == 'High']['status']\n",
    "\n",
    "# Perform ANOVA test\n",
    "f_stat, p_value = f_oneway(group_low, group_medium, group_high)\n",
    "print(f\"F-statistic: {f_stat}\")\n",
    "print(f\"P-value: {p_value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab16251f",
   "metadata": {},
   "source": [
    "The results indicate that there are statistically significant differences in the conversion rates for different levels of profile completion.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95429a3a",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "- Missing value treatment (if needed)\n",
    "- Feature engineering (if needed)\n",
    "- Outlier detection and treatment (if needed)\n",
    "- Preparing data for modeling\n",
    "- Any other preprocessing steps (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b3698c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# outlier detection using boxplot\n",
    "numeric_columns = data.select_dtypes(include=np.number).columns.tolist()\n",
    "# dropping release_year as it is a temporal variable\n",
    "numeric_columns.remove(\"status\")\n",
    "\n",
    "plt.figure(figsize=(15, 12))\n",
    "\n",
    "for i, variable in enumerate(numeric_columns):\n",
    "    plt.subplot(4, 4, i + 1)\n",
    "    plt.boxplot(data[variable], whis=1.5)\n",
    "    plt.tight_layout()\n",
    "    plt.title(variable)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9309e6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Engineering based on correlation for age and status and time-spent on website and status\n",
    "data['engagement_score'] = (data['website_visits'] +\n",
    "                                    data['time_spent_on_website'] +\n",
    "                                    data['page_views_per_visit'])\n",
    "\n",
    "data['age_engagement_interaction'] = data['age'] * data['engagement_score']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d295efd",
   "metadata": {},
   "source": [
    "Feature scaling is not necessary for tree-based models like Decision Trees and Random Forests because these models do not rely on the scale or distribution of the features. They make decisions based on order (which item is larger) rather than on the specific scale of the feature values, meaning that the varying scales of the raw data do not affect these models' performance.\n",
    "\n",
    "For decision tree models, including random forests, outliers will generally have less impact because these models are non-parametric—they do not make assumptions about the data distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "difficult-union",
   "metadata": {
    "id": "difficult-union"
   },
   "source": [
    "## EDA\n",
    "\n",
    "- It is a good idea to explore the data once again after manipulating it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interested-talent",
   "metadata": {
    "id": "interested-talent"
   },
   "outputs": [],
   "source": [
    "sns.violinplot(x='status', y='engagement_score', data=data)\n",
    "plt.title('Engagement Score by Lead Status')\n",
    "plt.xlabel('Lead Status')\n",
    "plt.ylabel('Engagement Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af31108f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data['age_engagement_interaction'], kde=True)\n",
    "plt.title('Distribution of Age-Engagement Interaction')\n",
    "plt.xlabel('Age-Engagement Interaction')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa843b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x='status', y='age_engagement_interaction', data=data)\n",
    "plt.title('Age-Engagement Interaction by Conversion Status')\n",
    "plt.xlabel('Conversion Status')\n",
    "plt.ylabel('Age-Engagement Interaction')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1d7ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['age_engagement_interaction'].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d44c252",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[['age_engagement_interaction', 'status']].corr())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2b3a65",
   "metadata": {},
   "source": [
    "Outlier detection, treatment, and Data-split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4061fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['age', 'website_visits', 'time_spent_on_website', 'page_views_per_visit', 'engagement_score',]:\n",
    "    # Calculate the 1st and 99th percentiles\n",
    "    lower_bound = data[col].quantile(0.01)\n",
    "    upper_bound = data[col].quantile(0.99)\n",
    "\n",
    "    # Cap the values\n",
    "    data[col] = np.clip(data[col], lower_bound, upper_bound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6dc0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop([\"status\"], axis=1)\n",
    "Y = data['status']  # Define the dependent (target) variable\n",
    "\n",
    "X = pd.get_dummies(X, drop_first=True) # Get dummies for X and avoid the dummy variable trap\n",
    "\n",
    "# Splitting the data in 70:30 ratio for train to test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, Y, test_size=0.30, random_state=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0805361b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of Training set : \", X_train.shape)\n",
    "print(\"Shape of test set : \", X_test.shape)\n",
    "print(\"Percentage of classes in training set:\")\n",
    "print(y_train.value_counts(normalize=True))\n",
    "print(\"Percentage of classes in test set:\")\n",
    "print(y_test.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amazing-fluid",
   "metadata": {
    "id": "amazing-fluid"
   },
   "source": [
    "## Building a Decision Tree model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neither-hydrogen",
   "metadata": {
    "id": "neither-hydrogen"
   },
   "outputs": [],
   "source": [
    "def metrics_score(actual, predicted):\n",
    "    print(classification_report(actual, predicted))\n",
    "    cm = confusion_matrix(actual, predicted)\n",
    "    plt.figure(figsize = (8, 5))\n",
    "    sns.heatmap(cm, annot = True,  fmt = '.2f', xticklabels = ['Not Converted', 'Converted'], yticklabels = ['Not Converted', 'Converted'])\n",
    "    plt.ylabel('Actual')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004d9189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the decision tree classifier on the training data\n",
    "d_tree = DecisionTreeClassifier(random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1be4442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the classifier on the training data\n",
    "d_tree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52098516",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train1 =  d_tree.predict(X_train)\n",
    "metrics_score(y_train, y_pred_train1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78ab758",
   "metadata": {},
   "source": [
    "The Decision tree is giving a 100% score for all metrics on the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffe35bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the type of classifier\n",
    "d_tree_tuned = DecisionTreeClassifier(random_state = 7, class_weight = {0: 0.3, 1: 0.7})\n",
    "\n",
    "# Grid of parameters to choose from\n",
    "parameters = {'max_depth': np.arange(2, 10),\n",
    "              'criterion': ['gini', 'entropy'],\n",
    "              'min_samples_leaf': [5, 10, 20, 25]\n",
    "             }\n",
    "\n",
    "# Type of scoring used to compare parameter combinations - recall score for class 1\n",
    "scorer = metrics.make_scorer(recall_score, pos_label = 1)\n",
    "\n",
    "# Run the grid search\n",
    "grid_obj = GridSearchCV(d_tree_tuned, parameters, scoring = scorer, cv = 5)\n",
    "\n",
    "grid_obj = grid_obj.fit(X_train, y_train)\n",
    "\n",
    "# Set the classifier to the best combination of parameters\n",
    "d_tree_tuned = grid_obj.best_estimator_\n",
    "\n",
    "# Fit the best algorithm to the data\n",
    "d_tree_tuned.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c167b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train2 = d_tree_tuned.predict(X_train)\n",
    "# Using the metrics_score function to evaluate the model's performance\n",
    "metrics_score(y_train, y_pred_train2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01daa088",
   "metadata": {},
   "source": [
    "The Decision Tree works well on the untuned training data but not so well on the tuned train data as the recall is 0.88 in comparison to 1 for the training dataset, i.e., the Decision Tree is overfitting the original training data. The precision on the tuned trained data suggests that there's a 38% (1 - 0.62) chance that the model will predict that a person is lead is going to convert even though he/she would not, and the company may waste their time and energy on these leads who are not at the brink of conversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018dce56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making predictions on the testing data with the tuned model\n",
    "y_pred_test2 = d_tree_tuned.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92254697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the metrics_score function to evaluate the model's performance on the test data\n",
    "metrics_score(y_test, y_pred_test2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f7664d",
   "metadata": {},
   "source": [
    "The Decision Tree works not so well on the tuned test data as the recall is 0.86 in comparison to 0.88 for the tuned training dataset, i.e., the Decision Tree is overfitting the training data. The precision on the tuned trained data suggests that there's a 38% (1 - 0.62) chance that the model will predict that a person is lead is going to convert even though he/she would not, and the company may waste their time and energy on these leads who are not at the brink of conversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073839f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = list(X.columns)\n",
    "\n",
    "plt.figure(figsize = (20, 20))\n",
    "\n",
    "tree.plot_tree(d_tree_tuned, feature_names = features, filled = True, fontsize = 9, node_ids = True, class_names = True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d7b9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importance of features in the tree building\n",
    "\n",
    "# Importance of features in the tree building\n",
    "\n",
    "print (pd.DataFrame(d_tree_tuned.feature_importances_, columns = [\"Imp\"], index = X_train.columns).sort_values(by = 'Imp', ascending = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e14d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the feature importance\n",
    "importances = d_tree_tuned.feature_importances_\n",
    "\n",
    "indices = np.argsort(importances)\n",
    "\n",
    "plt.figure(figsize = (10, 10))\n",
    "\n",
    "plt.title('Feature Importances')\n",
    "\n",
    "plt.barh(range(len(indices)), importances[indices], color = 'violet', align = 'center')\n",
    "\n",
    "plt.yticks(range(len(indices)), [features[i] for i in indices])\n",
    "\n",
    "plt.xlabel('Relative Importance')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8dea35",
   "metadata": {},
   "source": [
    "Observations:\n",
    "\n",
    "Time spent on the website and first_interaction_website are the most important features followed by profile_completed, age, and last_activity. The rest of the variables have no impact in this model, while deciding whether a lead will be converted or not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "limited-strengthening",
   "metadata": {
    "id": "limited-strengthening"
   },
   "source": [
    "## Do we need to prune the tree?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bc51e4",
   "metadata": {},
   "source": [
    "Yes, we should prune the tree since some features are not important at all in the decision tree model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ambient-elements",
   "metadata": {
    "id": "ambient-elements"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Search for the optimal ccp_alpha value\n",
    "alpha_values = np.linspace(0.001, 0.02, 50)\n",
    "mean_scores = []\n",
    "\n",
    "for ccp_alpha in alpha_values:\n",
    "    clf = DecisionTreeClassifier(random_state=7, ccp_alpha=ccp_alpha)\n",
    "    scores = cross_val_score(clf, X_train, y_train, cv=5, scoring='accuracy')\n",
    "    mean_scores.append(np.mean(scores))\n",
    "\n",
    "# Find the alpha value with the highest mean accuracy score\n",
    "optimal_ccp_alpha = alpha_values[np.argmax(mean_scores)]\n",
    "\n",
    "# Prune the tree using the optimal ccp_alpha\n",
    "d_tree_pruned = DecisionTreeClassifier(random_state=7, ccp_alpha=optimal_ccp_alpha)\n",
    "d_tree_pruned.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe920ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the mean accuracy scores over different ccp_alpha values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(alpha_values, mean_scores, marker='o', linestyle='--', color='b')\n",
    "plt.title('Mean Accuracy Score over different ccp_alpha values')\n",
    "plt.xlabel('ccp_alpha')\n",
    "plt.ylabel('Mean Accuracy')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241aae84",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = list(X.columns)\n",
    "\n",
    "plt.figure(figsize = (20, 20))\n",
    "\n",
    "tree.plot_tree(d_tree_pruned, feature_names = features, filled = True, fontsize = 9, node_ids = True, class_names = True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c47c07",
   "metadata": {
    "id": "amazing-fluid"
   },
   "source": [
    "## Building a Random Forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbd87fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_estimator = RandomForestClassifier(random_state=1)\n",
    "\n",
    "# Fit the classifier on the training data\n",
    "rf_estimator.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8fdd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making predictions on the training data with the random forest classifier\n",
    "y_pred_train3 = rf_estimator.predict(X_train)\n",
    "\n",
    "# Using the metrics_score function to evaluate the model's performance on the training data\n",
    "metrics_score(y_train, y_pred_train3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76431be4",
   "metadata": {},
   "source": [
    "Observation:\n",
    "\n",
    "For all the metrics in the training dataset, the Random Forest gives a 100% score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bdb89b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making predictions on the testing data with the random forest classifier\n",
    "y_pred_test3 = rf_estimator.predict(X_test)\n",
    "\n",
    "# Using the metrics_score function to evaluate the model's performance on the test data\n",
    "metrics_score(y_test, y_pred_test3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f6b772",
   "metadata": {},
   "source": [
    "The Random Forest classifier seems to be overfitting the training data. The recall on the training data is 1, while the recall on the test data is only ~ 0.68 for class 1. Precision is high for the test data as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e82859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the type of classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer, recall_score\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "# Setup the RandomForestClassifier with your specified criterion and random state\n",
    "rf_estimator_tuned = RandomForestClassifier(criterion=\"entropy\", random_state=7)\n",
    "\n",
    "# Define the grid of parameters to search over\n",
    "parameters = {\n",
    "    \"n_estimators\": [110, 120],\n",
    "    \"max_depth\": [6, 7],\n",
    "    \"min_samples_leaf\": [20, 25],\n",
    "    \"max_features\": [0.8, 0.9],\n",
    "    \"max_samples\": [0.9, 1],\n",
    "    \"class_weight\": [\"balanced\", {0: 0.3, 1: 0.7}]\n",
    "}\n",
    "\n",
    "# Define the scorer based on recall score for class 1\n",
    "scorer = make_scorer(recall_score, pos_label=1)\n",
    "\n",
    "# Setup GridSearchCV with the RandomForestClassifier, the grid of parameters, and the scoring method\n",
    "grid_obj = GridSearchCV(rf_estimator_tuned, parameters, scoring=scorer, cv=5, n_jobs=-1)\n",
    "\n",
    "# Fit the grid search object to the training data to search for the best parameters\n",
    "grid_obj.fit(X_train, y_train)\n",
    "\n",
    "# After the search, save the best estimator to rf_estimator_tuned\n",
    "rf_estimator_tuned = grid_obj.best_estimator_\n",
    "\n",
    "# If you'd like to print out the best parameters found, you can do so like this:\n",
    "print(\"Best parameters found: \", grid_obj.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ea0b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## random search\n",
    "##from sklearn.model_selection import RandomizedSearchCV\n",
    "##rf_estimator_tuned = RandomForestClassifier(criterion=\"entropy\", random_state=7)\n",
    "# Setup the randomized search with the same parameters and distributions\n",
    "##random_search = RandomizedSearchCV(rf_estimator_tuned, param_distributions=parameters, n_iter=10, scoring=scorer, cv=5, random_state=7, n_jobs=-1)\n",
    "\n",
    "# Fit the randomized search object to the training data\n",
    "##random_search.fit(X_train, y_train)\n",
    "\n",
    "# Save the best estimator to variable rf_estimator_tuned\n",
    "##rf_estimator_tuned = random_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3b1ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_estimator_tuned.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d0e3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making predictions on the training data with the tuned random forest classifier\n",
    "y_pred_train4 = rf_estimator_tuned.predict(X_train)\n",
    "\n",
    "# Using the metrics_score function to evaluate the model's performance on the training data\n",
    "metrics_score(y_train, y_pred_train4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce531da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making predictions on the test data with the tuned random forest classifier\n",
    "y_pred_test4 = rf_estimator_tuned.predict(X_test)\n",
    "\n",
    "# Using the metrics_score function to evaluate the model's performance on the test data\n",
    "metrics_score(y_test, y_pred_test4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e20ce0",
   "metadata": {},
   "source": [
    "Note that the tuned test dataset performs better than the tuned trained dataset in terms of precision for the class 1. The recall has reduced though by 0.03. Accuracy remains the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41a3c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = rf_estimator_tuned.feature_importances_\n",
    "\n",
    "indices = np.argsort(importances)\n",
    "\n",
    "feature_names = list(X.columns)\n",
    "\n",
    "plt.figure(figsize = (12, 12))\n",
    "\n",
    "plt.title('Feature Importances')\n",
    "\n",
    "plt.barh(range(len(indices)), importances[indices], color = 'violet', align = 'center')\n",
    "\n",
    "plt.yticks(range(len(indices)), [feature_names[i] for i in indices])\n",
    "\n",
    "plt.xlabel('Relative Importance')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af6eea7",
   "metadata": {},
   "source": [
    "Observations: Similar to the decision tree model, time spent on website, first_interaction_website, profile_completed, and age are the top four features that help distinguish between not converted and converted leads. Unlike the decision tree, the random forest gives some importance to other variables like occupation, page_views_per_visit, as well. This implies that the random forest is giving importance to more factors in comparison to the decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11bc14ca",
   "metadata": {},
   "source": [
    "## Do we need to prune the tree?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a4dc57",
   "metadata": {},
   "source": [
    "We don't need to prune the tree when considering the random forest trees because there is built-in regularization: Random Forest inherently includes several mechanisms that prevent overfitting despite the complexity of individual trees. This is achieved through the randomness introduced by selecting different subsets of features and training examples. The ensemble method, where predictions are averaged (for regression) or voted upon (for classification), further mitigates the risk of overfitting.\n",
    "\n",
    "Also getting the results, we can see that each variable is holding some importance in the decision making process here, so no need to prune the tree.\n",
    "\n",
    "However, the above is true only if we used random search instead of GridSearch. Since we used GridSearch, we have certain features that are not that important, hence we can prune the tree. To further prune the tree we can only change the tuning of individual parameters like we did before."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nasty-retailer",
   "metadata": {
    "id": "nasty-retailer"
   },
   "source": [
    "## Actionable Insights and Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f65413c",
   "metadata": {},
   "source": [
    "We saw that time_spent_on_website is the most important feature when considering the lead status in the decision tree model and the first_interaction feature is the most important in the random forest model. Since time_spent_on_website is a crucial factor, ensuring that the website is engaging, informative, and easy to navigate can encourage potential leads to spend more time exploring the content.\n",
    "The company should focus marketing efforts and budgets on the channels that have the highest lead conversion rate.\n",
    "They should focus on user feedback for designing their recommendation systems further and employ A/B testing for experimenting with different strategies.\n",
    "The insights regarding the statistical tests are visualized via graphs and commented upon as well in the document. We have used ANOVA and Chi-squared test to judge the impact of these factors.\n",
    "We have gauged the model predictions through decision trees and random forests and have visualized and commented upon its findings.\n",
    "Further we have included feature engineering (adding two new features) and performed EDA on the dataset before and after adding the features\n",
    "we have tuned the model to deal with overfitting and used classification reports to measure the precion, recall, f1 score, accuracy and confusion matrix of the variables\n",
    "we created 2 lists: a numerical one and a categorical one to proceed with our analysis on the same and have gauged the target variable wrt all the other columns through our univariate, bivariate, and multivariate analysis.\n",
    "We determined the skewness of the distributions of the numeric variables where we learnt that the age factor is skewed left. The central tendency of points, the shape and other factors.\n",
    "We also focused on important features at the end of predictive models and decided upon their pruning status."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ExtraaLearn_Project_Template_Notebook.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
